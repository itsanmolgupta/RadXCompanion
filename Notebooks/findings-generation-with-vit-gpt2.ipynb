{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T14:27:37.526764Z",
     "iopub.status.busy": "2025-01-22T14:27:37.526349Z",
     "iopub.status.idle": "2025-01-22T14:27:48.612398Z",
     "shell.execute_reply": "2025-01-22T14:27:48.610887Z",
     "shell.execute_reply.started": "2025-01-22T14:27:37.526722Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "\n",
    "    AutoFeatureExtractor, \n",
    "\n",
    "    AutoTokenizer, \n",
    "\n",
    "    VisionEncoderDecoderModel,\n",
    "\n",
    "    Seq2SeqTrainingArguments,\n",
    "\n",
    "    Seq2SeqTrainer, \n",
    "\n",
    "    default_data_collator)\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import nltk\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "\n",
    "# remove warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-20T15:35:50.057567Z",
     "iopub.status.busy": "2025-01-20T15:35:50.056229Z",
     "iopub.status.idle": "2025-01-20T15:36:02.496136Z",
     "shell.execute_reply": "2025-01-20T15:36:02.495258Z",
     "shell.execute_reply.started": "2025-01-20T15:35:50.057529Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2SdpaAttention(\n",
       "            (c_attn): Conv1D(nf=2304, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2SdpaAttention(\n",
       "            (c_attn): Conv1D(nf=1536, nx=768)\n",
       "            (q_attn): Conv1D(nf=768, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=3072, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=3072)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T14:27:57.168890Z",
     "iopub.status.busy": "2025-01-22T14:27:57.168055Z",
     "iopub.status.idle": "2025-01-22T14:28:13.589458Z",
     "shell.execute_reply": "2025-01-22T14:28:13.588068Z",
     "shell.execute_reply.started": "2025-01-22T14:27:57.168807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd8c301e1dc486d878b2fa08d43d3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04505f4301a648d9b7d231f516521695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00002.parquet:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b1f8b3570843378cc950e326b71166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00002.parquet:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11f2349a93e497f846eb34b9a741373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/19455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Example: Loading dataset from Hugging Face Hub (adjust as needed)\n",
    "ds = load_dataset(\"\", split=\"train\")  # Replace with your dataset and split\n",
    "\n",
    "# Split the dataset into train and temp (for validation and test)\n",
    "train_val_split = 0.80\n",
    "train_ds, temp_ds = ds.train_test_split(test_size=1 - train_val_split).values()\n",
    "\n",
    "# Split the temp dataset into validation and test sets\n",
    "val_size = 0.5\n",
    "val_ds, test_ds = temp_ds.train_test_split(test_size=val_size).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T14:28:15.023584Z",
     "iopub.status.busy": "2025-01-22T14:28:15.023158Z",
     "iopub.status.idle": "2025-01-22T14:28:15.028911Z",
     "shell.execute_reply": "2025-01-22T14:28:15.027333Z",
     "shell.execute_reply.started": "2025-01-22T14:28:15.023547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "num_beams = 4\n",
    "\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T15:36:18.540603Z",
     "iopub.status.busy": "2025-01-20T15:36:18.540248Z",
     "iopub.status.idle": "2025-01-20T15:36:19.969321Z",
     "shell.execute_reply": "2025-01-20T15:36:19.968563Z",
     "shell.execute_reply.started": "2025-01-20T15:36:18.540578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Findings: a black and white photo of a person holding a black and white vase\n",
      "Actual Findings: The lungs are clear of focal consolidation, pleural effusion or pneumothorax. The heart size is normal. The mediastinal contours are normal. Multiple surgical clips project over the left breast, and old left rib fractures are noted.\n",
      "-----\n",
      "Generated Findings: a black and white photo of a black and white photo\n",
      "Actual Findings: Lung volumes remain low. There are innumerable bilateral scattered small pulmonary nodules which are better demonstrated on recent CT. Mild pulmonary vascular congestion is stable. The cardiomediastinal silhouette and hilar contours are unchanged. Small pleural effusion in the right middle fissure is new. There is no new focal opacity to suggest pneumonia. There is no pneumothorax.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "def predict_step(dataset, indices):\n",
    "    images = []\n",
    "    actual_captions = []  # List to store actual captions\n",
    "\n",
    "    for idx in indices:\n",
    "        i_image = dataset[idx]['image']\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "        images.append(i_image)\n",
    "        actual_captions.append(dataset[idx]['findings'])  # Add actual caption to the list\n",
    "\n",
    "    # Extract pixel values and attention masks\n",
    "    inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "    pixel_values = inputs.pixel_values.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device) if 'attention_mask' in inputs else None\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 512, \"num_beams\": 4}  # Adjust parameters as needed\n",
    "\n",
    "    # Include attention_mask in generate call if available\n",
    "    if attention_mask is not None:\n",
    "        output_ids = model.generate(pixel_values, attention_mask=attention_mask, **gen_kwargs)\n",
    "    else:\n",
    "        output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "\n",
    "    return preds, actual_captions  # Return both predictions and actual captions\n",
    "\n",
    "# Example usage: predict captions for the first two images in the dataset\n",
    "predictions, actual_captions = predict_step(ds, [0, 1])\n",
    "\n",
    "for pred, actual in zip(predictions, actual_captions):\n",
    "    print(f\"Generated Findings: {pred}\")\n",
    "    print(f\"Actual Findings: {actual}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T15:36:26.105778Z",
     "iopub.status.busy": "2025-01-20T15:36:26.105428Z",
     "iopub.status.idle": "2025-01-20T15:41:03.558225Z",
     "shell.execute_reply": "2025-01-20T15:41:03.557245Z",
     "shell.execute_reply.started": "2025-01-20T15:36:26.105752Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a4574b04c2462cb7d8709d566c2884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15564 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b0bf669ee14acab1d88b0526224869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f79cc361e3b49d3a23b53030eeef0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: 15564\n",
      "Validation dataset shape: 1945\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to prepare the dataset for training\n",
    "\n",
    "def preprocess_data(examples):\n",
    "\n",
    "    # Convert images to RGB and apply feature extractor\n",
    "\n",
    "    images = [i.convert(\"RGB\") if i.mode != \"RGB\" else i for i in examples['image']]\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    text_inputs = tokenizer(examples['findings'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    text_inputs['labels'] = text_inputs.input_ids\n",
    "\n",
    "    text_inputs['pixel_values'] = pixel_values\n",
    "\n",
    "\n",
    "\n",
    "    return text_inputs\n",
    "\n",
    "\n",
    "# Preprocess the datasets\n",
    "\n",
    "train_dataset = train_ds.map(preprocess_data, batched=True, remove_columns=['image', 'findings', 'impression'])\n",
    "\n",
    "val_dataset = val_ds.map(preprocess_data, batched=True, remove_columns=['image', 'findings', 'impression'])\n",
    "\n",
    "test_dataset = test_ds.map(preprocess_data, batched=True, remove_columns=['image', 'findings', 'impression'])\n",
    "\n",
    "\n",
    "\n",
    "# Print dataset sizes to check\n",
    "\n",
    "print(f\"Train dataset shape: {len(train_dataset)}\")\n",
    "\n",
    "print(f\"Validation dataset shape: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T05:43:50.601147Z",
     "iopub.status.busy": "2025-01-20T05:43:50.600801Z",
     "iopub.status.idle": "2025-01-20T05:43:52.278127Z",
     "shell.execute_reply": "2025-01-20T05:43:52.277371Z",
     "shell.execute_reply.started": "2025-01-20T05:43:50.601117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T05:43:54.498687Z",
     "iopub.status.busy": "2025-01-20T05:43:54.498381Z",
     "iopub.status.idle": "2025-01-20T05:44:07.506981Z",
     "shell.execute_reply": "2025-01-20T05:44:07.506295Z",
     "shell.execute_reply.started": "2025-01-20T05:43:54.498665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mitsanmolgupta\u001b[0m (\u001b[33manmol_creations\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250120_054401-ji56cz7q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anmol_creations/findings_generation/runs/ji56cz7q' target=\"_blank\">vit-findings-generation</a></strong> to <a href='https://wandb.ai/anmol_creations/findings_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anmol_creations/findings_generation' target=\"_blank\">https://wandb.ai/anmol_creations/findings_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anmol_creations/findings_generation/runs/ji56cz7q' target=\"_blank\">https://wandb.ai/anmol_creations/findings_generation/runs/ji56cz7q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/anmol_creations/findings_generation/runs/ji56cz7q?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4da85061d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb_api = \"\"\n",
    "wandb.login(key=wandb_api)\n",
    "wandb.init(project=\"findings_generation\",\n",
    "          name=\"vit-findings-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T05:44:14.709383Z",
     "iopub.status.busy": "2025-01-20T05:44:14.709027Z",
     "iopub.status.idle": "2025-01-20T14:36:24.104130Z",
     "shell.execute_reply": "2025-01-20T14:36:24.102718Z",
     "shell.execute_reply.started": "2025-01-20T05:44:14.709358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19460' max='19460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19460/19460 8:52:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.206469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>0.192712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>0.186247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.182160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.180814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.181140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.157700</td>\n",
       "      <td>0.183833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.139300</td>\n",
       "      <td>0.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>0.191353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.196180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19460, training_loss=0.1725445248485224, metrics={'train_runtime': 31928.8051, 'train_samples_per_second': 4.875, 'train_steps_per_second': 0.609, 'total_flos': 2.808740001393672e+19, 'train_loss': 0.1725445248485224, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"./output\",\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    \n",
    "    per_device_eval_batch_size=8,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "\n",
    "    num_train_epochs=10,  # Adjust as needed\n",
    "\n",
    "    save_steps=500,\n",
    "\n",
    "    save_total_limit=2,\n",
    "\n",
    "    logging_dir=\"./logs\",\n",
    "\n",
    "    logging_steps=100,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define a data collator\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "\n",
    "\n",
    "# Create Trainer instance\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "    model=model,\n",
    "\n",
    "    args=training_args,\n",
    "\n",
    "    train_dataset=train_dataset,\n",
    "\n",
    "    data_collator=data_collator,\n",
    "\n",
    "    eval_dataset=val_dataset,  # Pass the validation dataset here\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Fine-tune the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:41:01.796984Z",
     "iopub.status.busy": "2025-01-20T14:41:01.796540Z",
     "iopub.status.idle": "2025-01-20T14:41:08.952097Z",
     "shell.execute_reply": "2025-01-20T14:41:08.951287Z",
     "shell.execute_reply.started": "2025-01-20T14:41:01.796942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge-score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-20T14:41:11.652656Z",
     "iopub.status.busy": "2025-01-20T14:41:11.652319Z",
     "iopub.status.idle": "2025-01-20T14:41:18.922273Z",
     "shell.execute_reply": "2025-01-20T14:41:18.921447Z",
     "shell.execute_reply.started": "2025-01-20T14:41:11.652631Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.5\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (1.4.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (4.67.1)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=eb712358d979514e65d9abcd91e57da22f1755ab7562cd096066a03a21c0932f\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/ab/82/f9667f6f884d272670a15382599a9c753a1dfdc83f7412e37d\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nltk-3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T15:23:18.856443Z",
     "iopub.status.busy": "2025-01-20T15:23:18.856009Z",
     "iopub.status.idle": "2025-01-20T15:23:23.078505Z",
     "shell.execute_reply": "2025-01-20T15:23:23.077545Z",
     "shell.execute_reply.started": "2025-01-20T15:23:18.856400Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T14:34:49.697754Z",
     "iopub.status.busy": "2025-01-22T14:34:49.697342Z",
     "iopub.status.idle": "2025-01-22T14:35:12.992291Z",
     "shell.execute_reply": "2025-01-22T14:35:12.990972Z",
     "shell.execute_reply.started": "2025-01-22T14:34:49.697720Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Findings: \n",
      "AP portable upright view of the chest. Left chest wall AICD is again seen with leads extending to the region of the right atrium and right ventricle. The heart remains markedly enlarged. There is mild elevation of the right hemidiaphragm again noted. At least mild pulmonary edema is noted. No large effusion or pneumothorax. Mediastinal contour is unchanged. Bony structures are intact peer\n",
      "-----\n",
      "Generated Findings: \n",
      "AP portable upright view of the chest. Left chest wall AICD is again seen with leads extending to the region the right atrium and right ventricle. Cardiomegaly is again noted with mild pulmonary edema. No large effusion or pneumothorax. Bony structures are intact.\n",
      "-----\n",
      "BLEU score: 0.4881\n",
      "ROUGE1 score: 0.7736\n",
      "ROUGE2 score: 0.6538\n",
      "ROUGEL score: 0.7736\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Ensure you have the necessary NLTK data files\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def predict_step(dataset, indices):\n",
    "    images = []\n",
    "    actual_captions = []  # List to store actual captions\n",
    "    for idx in indices:\n",
    "        i_image = dataset[idx]['image']\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "        images.append(i_image)\n",
    "        actual_captions.append(dataset[idx]['findings'])  # Add actual caption to the list\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 512, \"num_beams\": 4}  # Adjust parameters as needed\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "\n",
    "    return preds, actual_captions  # Return both predictions and actual captions\n",
    "\n",
    "# Example usage: predict captions for the first two images in the dataset\n",
    "predictions, actual_captions = predict_step(train_ds, [100])\n",
    "\n",
    "# Evaluate BLEU score\n",
    "bleu_scores = []\n",
    "\n",
    "for pred, actual in zip(predictions, actual_captions):\n",
    "    # Tokenize the sentences\n",
    "    reference = [word_tokenize(actual.lower())]  # Reference captions (list of lists)\n",
    "    candidate = word_tokenize(pred.lower())       # Generated captions (list)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "# Evaluate ROUGE score\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "for pred, actual in zip(predictions, actual_captions):\n",
    "    scores = rouge.score(actual, pred)\n",
    "    for key in rouge_scores:\n",
    "        rouge_scores[key].append(scores[key].fmeasure)\n",
    "\n",
    "# Print predictions, actual captions, BLEU scores, and ROUGE scores\n",
    "for idx, (pred, actual, bleu_score) in enumerate(zip(predictions, actual_captions, bleu_scores)):\n",
    "    print(f\"Actual Findings: \\n{actual}\")\n",
    "    print(\"-----\")\n",
    "    print(f\"Generated Findings: \\n{pred}\")\n",
    "    print(\"-----\")\n",
    "    print(f\"BLEU score: {bleu_score:.4f}\")\n",
    "    for rouge_key in rouge_scores:\n",
    "        print(f\"{rouge_key.upper()} score: {rouge_scores[rouge_key][idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T14:39:59.876056Z",
     "iopub.status.busy": "2025-01-22T14:39:59.875542Z",
     "iopub.status.idle": "2025-01-22T14:40:40.032197Z",
     "shell.execute_reply": "2025-01-22T14:40:40.030936Z",
     "shell.execute_reply.started": "2025-01-22T14:39:59.876014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Findings: \n",
      "Portable single frontal chest radiograph was obtained with the patient in semi-upright position. There has been interval increase in the opacity projecting over the left hemithorax. There is complete opacification of the left lung base with air bronchograms and obscuration of the left hemidiaphragm. There has also been interval increase in the right base opacity. There is no pneumothorax. The heart size is difficult to assess given parenchymal abnormalities. \n",
      "-----\n",
      "Generated Findings: \n",
      "There has been interval placement of a left internal jugular central venous catheter with tip terminating in the mid SVC. The tip of the endotracheal tube projects over the mid thoracic trachea. The tip of the enteric tube projects below the level of the diaphragm but beyond the field of view of this radiograph. There is persistent opacification of the left lung base with opacification of the right lung base likely reflecting a combination of pleural effusion and atelectasis. There is no pneumothorax identified. The appearance of the cardiomediastinal silhouette is unchanged.\n",
      "-----\n",
      "BLEU score: 0.1622\n",
      "ROUGE1 score: 0.4198\n",
      "ROUGE2 score: 0.2250\n",
      "ROUGEL score: 0.3457\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Ensure you have the necessary NLTK data files\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def predict_step(dataset, indices):\n",
    "    images = []\n",
    "    actual_captions = []  # List to store actual captions\n",
    "    for idx in indices:\n",
    "        i_image = dataset[idx]['image']\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "        images.append(i_image)\n",
    "        actual_captions.append(dataset[idx]['findings'])  # Add actual caption to the list\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 512, \"num_beams\": 4}  # Adjust parameters as needed\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "\n",
    "    return preds, actual_captions  # Return both predictions and actual captions\n",
    "\n",
    "# Example usage: predict captions for the first two images in the dataset\n",
    "predictions, actual_captions = predict_step(train_ds, [500])\n",
    "\n",
    "# Evaluate BLEU score\n",
    "bleu_scores = []\n",
    "\n",
    "for pred, actual in zip(predictions, actual_captions):\n",
    "    # Tokenize the sentences\n",
    "    reference = [word_tokenize(actual.lower())]  # Reference captions (list of lists)\n",
    "    candidate = word_tokenize(pred.lower())       # Generated captions (list)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "# Evaluate ROUGE score\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "for pred, actual in zip(predictions, actual_captions):\n",
    "    scores = rouge.score(actual, pred)\n",
    "    for key in rouge_scores:\n",
    "        rouge_scores[key].append(scores[key].fmeasure)\n",
    "\n",
    "# Print predictions, actual captions, BLEU scores, and ROUGE scores\n",
    "for idx, (pred, actual, bleu_score) in enumerate(zip(predictions, actual_captions, bleu_scores)):\n",
    "    print(f\"Actual Findings: \\n{actual}\")\n",
    "    print(\"-----\")\n",
    "    print(f\"Generated Findings: \\n{pred}\")\n",
    "    print(\"-----\")\n",
    "    print(f\"BLEU score: {bleu_score:.4f}\")\n",
    "    for rouge_key in rouge_scores:\n",
    "        print(f\"{rouge_key.upper()} score: {rouge_scores[rouge_key][idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T14:30:27.667645Z",
     "iopub.status.busy": "2025-01-22T14:30:27.667163Z",
     "iopub.status.idle": "2025-01-22T14:30:35.677700Z",
     "shell.execute_reply": "2025-01-22T14:30:35.676113Z",
     "shell.execute_reply.started": "2025-01-22T14:30:27.667612Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu rouge_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T17:49:21.962366Z",
     "iopub.status.busy": "2025-01-20T17:49:21.961942Z",
     "iopub.status.idle": "2025-01-20T20:48:44.139945Z",
     "shell.execute_reply": "2025-01-20T20:48:44.139020Z",
     "shell.execute_reply.started": "2025-01-20T17:49:21.962336Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Evaluation:\n",
      "Average BLEU: 13.8785\n",
      "Average ROUGE1: 0.4338\n",
      "Average ROUGE2: 0.1990\n",
      "Average ROUGEL: 0.3156\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "from rouge_score import rouge_scorer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize BLEU scorer\n",
    "bleu = BLEU()\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def custom_collate_fn(batch):\n",
    "    images = []\n",
    "    findings = []\n",
    "    for item in batch:\n",
    "        # Convert image to RGB\n",
    "        img = item['image']\n",
    "        img = img.convert(\"RGB\") if img.mode != \"RGB\" else img\n",
    "        images.append(img)\n",
    "        findings.append(item['findings'])\n",
    "    return {'images': images, 'findings': findings}\n",
    "\n",
    "def evaluate_test_set(dataset, batch_size=8):\n",
    "    \"\"\"\n",
    "    Evaluate BLEU and ROUGE scores on the test set with minimal RAM usage.\n",
    "\n",
    "    Args:\n",
    "        dataset: The test dataset.\n",
    "        batch_size: Number of samples to process in each batch.\n",
    "    \n",
    "    Returns:\n",
    "        None. Prints average BLEU and ROUGE scores.\n",
    "    \"\"\"\n",
    "    # Create DataLoader for batch processing with custom collate function\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    bleu_scores = []\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Extract images and findings\n",
    "        images = batch['images']\n",
    "        actuals = batch['findings']\n",
    "\n",
    "        # Predict findings for the batch\n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n",
    "        pixel_values = inputs.pixel_values\n",
    "        output_ids = model.generate(pixel_values, max_length=512, num_beams=4)\n",
    "        predictions = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        predictions = [pred.strip() for pred in predictions]\n",
    "\n",
    "        # Compute scores for the batch\n",
    "        for pred, actual in zip(predictions, actuals):\n",
    "            # BLEU score\n",
    "            bleu_score = bleu.sentence_score(pred, [actual]).score\n",
    "            bleu_scores.append(bleu_score)\n",
    "\n",
    "            # ROUGE scores\n",
    "            rouge_result = rouge.score(pred, actual)\n",
    "            for key in rouge_scores.keys():\n",
    "                rouge_scores[key].append(rouge_result[key].fmeasure)\n",
    "\n",
    "    # Print average scores\n",
    "    print(\"Train Set Evaluation:\")\n",
    "    print(f\"Average BLEU: {sum(bleu_scores) / len(bleu_scores):.4f}\")\n",
    "    for key in rouge_scores.keys():\n",
    "        print(f\"Average {key.upper()}: {sum(rouge_scores[key]) / len(rouge_scores[key]):.4f}\")\n",
    "    print(\"-----\")\n",
    "\n",
    "# Call the function to evaluate the test set\n",
    "evaluate_test_set(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T16:27:36.414251Z",
     "iopub.status.busy": "2025-01-20T16:27:36.413885Z",
     "iopub.status.idle": "2025-01-20T16:50:44.905682Z",
     "shell.execute_reply": "2025-01-20T16:50:44.904864Z",
     "shell.execute_reply.started": "2025-01-20T16:27:36.414222Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation:\n",
      "Average BLEU: 13.3225\n",
      "Average ROUGE1: 0.4295\n",
      "Average ROUGE2: 0.1944\n",
      "Average ROUGEL: 0.3117\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "from rouge_score import rouge_scorer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize BLEU scorer\n",
    "bleu = BLEU()\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def custom_collate_fn(batch):\n",
    "    images = []\n",
    "    findings = []\n",
    "    for item in batch:\n",
    "        # Convert image to RGB\n",
    "        img = item['image']\n",
    "        img = img.convert(\"RGB\") if img.mode != \"RGB\" else img\n",
    "        images.append(img)\n",
    "        findings.append(item['findings'])\n",
    "    return {'images': images, 'findings': findings}\n",
    "\n",
    "def evaluate_test_set(dataset, batch_size=8):\n",
    "    \"\"\"\n",
    "    Evaluate BLEU and ROUGE scores on the test set with minimal RAM usage.\n",
    "\n",
    "    Args:\n",
    "        dataset: The test dataset.\n",
    "        batch_size: Number of samples to process in each batch.\n",
    "    \n",
    "    Returns:\n",
    "        None. Prints average BLEU and ROUGE scores.\n",
    "    \"\"\"\n",
    "    # Create DataLoader for batch processing with custom collate function\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    bleu_scores = []\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Extract images and findings\n",
    "        images = batch['images']\n",
    "        actuals = batch['findings']\n",
    "\n",
    "        # Predict findings for the batch\n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n",
    "        pixel_values = inputs.pixel_values\n",
    "        output_ids = model.generate(pixel_values, max_length=512, num_beams=4)\n",
    "        predictions = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        predictions = [pred.strip() for pred in predictions]\n",
    "\n",
    "        # Compute scores for the batch\n",
    "        for pred, actual in zip(predictions, actuals):\n",
    "            # BLEU score\n",
    "            bleu_score = bleu.sentence_score(pred, [actual]).score\n",
    "            bleu_scores.append(bleu_score)\n",
    "\n",
    "            # ROUGE scores\n",
    "            rouge_result = rouge.score(pred, actual)\n",
    "            for key in rouge_scores.keys():\n",
    "                rouge_scores[key].append(rouge_result[key].fmeasure)\n",
    "\n",
    "    # Print average scores\n",
    "    print(\"Validation Set Evaluation:\")\n",
    "    print(f\"Average BLEU: {sum(bleu_scores) / len(bleu_scores):.4f}\")\n",
    "    for key in rouge_scores.keys():\n",
    "        print(f\"Average {key.upper()}: {sum(rouge_scores[key]) / len(rouge_scores[key]):.4f}\")\n",
    "    print(\"-----\")\n",
    "\n",
    "# Call the function to evaluate the test set\n",
    "evaluate_test_set(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T16:02:15.272619Z",
     "iopub.status.busy": "2025-01-20T16:02:15.272211Z",
     "iopub.status.idle": "2025-01-20T16:25:08.602152Z",
     "shell.execute_reply": "2025-01-20T16:25:08.601425Z",
     "shell.execute_reply.started": "2025-01-20T16:02:15.272594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Evaluation:\n",
      "Average BLEU: 13.4792\n",
      "Average ROUGE1: 0.4291\n",
      "Average ROUGE2: 0.1949\n",
      "Average ROUGEL: 0.3121\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "from rouge_score import rouge_scorer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize BLEU scorer\n",
    "bleu = BLEU()\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def custom_collate_fn(batch):\n",
    "    images = []\n",
    "    findings = []\n",
    "    for item in batch:\n",
    "        # Convert image to RGB\n",
    "        img = item['image']\n",
    "        img = img.convert(\"RGB\") if img.mode != \"RGB\" else img\n",
    "        images.append(img)\n",
    "        findings.append(item['findings'])\n",
    "    return {'images': images, 'findings': findings}\n",
    "\n",
    "def evaluate_test_set(dataset, batch_size=8):\n",
    "    \"\"\"\n",
    "    Evaluate BLEU and ROUGE scores on the test set with minimal RAM usage.\n",
    "\n",
    "    Args:\n",
    "        dataset: The test dataset.\n",
    "        batch_size: Number of samples to process in each batch.\n",
    "    \n",
    "    Returns:\n",
    "        None. Prints average BLEU and ROUGE scores.\n",
    "    \"\"\"\n",
    "    # Create DataLoader for batch processing with custom collate function\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    bleu_scores = []\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Extract images and findings\n",
    "        images = batch['images']\n",
    "        actuals = batch['findings']\n",
    "\n",
    "        # Predict findings for the batch\n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n",
    "        pixel_values = inputs.pixel_values\n",
    "        output_ids = model.generate(pixel_values, max_length=512, num_beams=4)\n",
    "        predictions = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        predictions = [pred.strip() for pred in predictions]\n",
    "\n",
    "        # Compute scores for the batch\n",
    "        for pred, actual in zip(predictions, actuals):\n",
    "            # BLEU score\n",
    "            bleu_score = bleu.sentence_score(pred, [actual]).score\n",
    "            bleu_scores.append(bleu_score)\n",
    "\n",
    "            # ROUGE scores\n",
    "            rouge_result = rouge.score(pred, actual)\n",
    "            for key in rouge_scores.keys():\n",
    "                rouge_scores[key].append(rouge_result[key].fmeasure)\n",
    "\n",
    "    # Print average scores\n",
    "    print(\"Test Set Evaluation:\")\n",
    "    print(f\"Average BLEU: {sum(bleu_scores) / len(bleu_scores):.4f}\")\n",
    "    for key in rouge_scores.keys():\n",
    "        print(f\"Average {key.upper()}: {sum(rouge_scores[key]) / len(rouge_scores[key]):.4f}\")\n",
    "    print(\"-----\")\n",
    "\n",
    "# Call the function to evaluate the test set\n",
    "evaluate_test_set(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:36:42.189596Z",
     "iopub.status.busy": "2025-01-20T14:36:42.189279Z",
     "iopub.status.idle": "2025-01-20T14:36:45.011165Z",
     "shell.execute_reply": "2025-01-20T14:36:45.010458Z",
     "shell.execute_reply.started": "2025-01-20T14:36:42.189571Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./findings-model/tokenizer_config.json',\n",
       " './findings-model/special_tokens_map.json',\n",
       " './findings-model/vocab.json',\n",
       " './findings-model/merges.txt',\n",
       " './findings-model/added_tokens.json',\n",
       " './findings-model/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "\n",
    "model.save_pretrained(\"./findings-model\")\n",
    "\n",
    "feature_extractor.save_pretrained(\"./findings-model\")\n",
    "\n",
    "tokenizer.save_pretrained(\"./findings-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:37:26.831783Z",
     "iopub.status.busy": "2025-01-20T14:37:26.831467Z",
     "iopub.status.idle": "2025-01-20T14:38:15.698250Z",
     "shell.execute_reply": "2025-01-20T14:38:15.697278Z",
     "shell.execute_reply.started": "2025-01-20T14:37:26.831758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='./findings_model-vitgpt2.zip' target='_blank'>./findings_model-vitgpt2.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/findings_model-vitgpt2.zip"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Define the directory to zip\n",
    "\n",
    "model_dir = \"./findings-model\"\n",
    "\n",
    "zip_file_path = \"./findings-model-vitgpt2.zip\"\n",
    "\n",
    "\n",
    "\n",
    "# Create a zip file of the model directory\n",
    "\n",
    "shutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', model_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Now, you can use the appropriate method to download the zip file.\n",
    "\n",
    "# If you're in a Jupyter Notebook, you can use the following code to create a download link:\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import FileLink\n",
    "\n",
    "\n",
    "\n",
    "# Create a download link for the zip file\n",
    "\n",
    "FileLink(zip_file_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 225320,
     "modelInstanceId": 203597,
     "sourceId": 238398,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
